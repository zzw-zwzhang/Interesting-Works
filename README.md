# Interesting-Works


## SGD
- [On the Origin of Implicit Regularization in Stochastic Gradient Descent](https://arxiv.org/pdf/2101.12176.pdf), 2021 ICLR
- [On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)](https://arxiv.org/pdf/2102.12470.pdf), 2021 NeurIPS
- [Rethinking the Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations, and Anomalous Diffusion](https://arxiv.org/pdf/2107.09133.pdf), 2021 NeurIPS
  1) Other works have questionedthe correctness of the using the central limit theorem to model the gradient noise as Gaussian [20], arguing that the weak dependence between batches and heavy-tailed structure in the gradient noiseleads the CLT to break down.

- [On the Interplay between Noise and Curvature and its Effecton Optimization and Generalization](https://arxiv.org/pdf/1906.07774.pdf), 2020 AISTATS
  1) The relationship between the Hessian and the covariance matrix. $Î£(\sita) \approx \frac{\sigma^2}{N} \sum_{i=1}^{N} x_i x_i^T = \frac{\sigma^2}{N} X^T X = \sigma^2 H$
- [Three Factors Influencing Minima in SGD](https://arxiv.org/pdf/1711.04623.pdf), 2018 ICANN
- [Entropy-SGD: Biasing Gradient Descent Into Wide Valleys](https://arxiv.org/pdf/1611.01838.pdf), 2017 ICLR
